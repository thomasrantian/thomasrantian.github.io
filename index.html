<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ran (Thomas) Tian</title>

  <meta name="author" content="Thomas Tian">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/favicon.ico">

</head>

<div class="header"
  style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-bottom:0px">
  <!-- <a href="index.html" class="logo" style="color:black;font-size:18px;">Andrea Bajcsy</a> -->
  <!-- <a href="index.html" class="logo" style="color:black;font-size:18px; padding-top:30px"><img src="images/intent_logo.png" height="80"></a> -->
  <div class="header-left" style="padding-left:1.0%;">
    <a href="index.html" style="text-align:left;font-size: 25px;padding-top:50px;color:black">Ran (Thomas) Tian - 田然</a>
  </div>
  <!-- <div class="header-right" style="padding-top:50px;">
    <a href="" style="font-size: 18px; color:black;">Blogs</a>
    <a href="" style="font-size: 18px; color:black">Full
      Publications</a>
  </div> -->
</div>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <!-- <hr> -->
              <tr style="padding:0px;text-align:left">
                <td style="padding-left:2.5%;padding-right:2.5%;padding-top:2.5%;width:63%;vertical-align:middle">
                  <!-- <p style="text-align:left">
                    <name>Ran (Thomas) Tian - 田然</name>
                    <br>
                    rantian [at] berkeley [dot] edu
                  </p> -->
                  <p>
                    I am a PhD student at <a href="https://bair.berkeley.edu//">UC Berkeley</a>
                    advised by <a href="https://msc.berkeley.edu/">Prof. Masayoshi Tomizuka</a>
                    and <a href="https://cmu-intentlab.github.io/">Prof. Andrea Bajcsy</a> at Carnegie Mellon
                    University.
                  </p>

                  <p>
                    My research lies in the intersection of robotics and AI with a focus on
                    safe alignment between embodied agents and humans.
                    I tackle the alignment and safety problems that emerge throughout the
                    life-cycle of foundation models in robotics, ranging from: training (wherein we need to collect
                    and quantify what kinds of embodied data will enable the desired robotics capabilities), to
                    fine-tuning (wherein we must align these models with humans), to deployment (where these models
                    must run in real-time, reliably detect out-of-distribution scenarios, and confidently hand over
                    control to fallback-strategies).
                    I ground my work through a variety of applications, from autonomous cars, to personalized robots, to
                    generative AI and in experiments with real human participants.
                  </p>

                  <p>
                    During my PhD study, I also spent a significant amount of time at <a
                      href="https://waymo.com/research/">Waymo</a>, scaling my research outcomes in driving foundation
                    models, including pre-training, post-training preference alignment, and distillation for onboard
                    deployment.
                    I am fortunate to have the opportunity to work at <a
                      href="https://research.nvidia.com/labs/avg/">NVIDIA Research</a>, focusing on
                    vision-language-action models for autonomous driving.
                    <!-- I spent Summer 2023 at <a href="https://waymo.com/research/">Waymo</a> 
                working on large autoregressive model for autonomous vehicle motion generation and efficient deployment.
                I also spent Summer 2022 at <a href="https://waymo.com/research/">Waymo</a> 
                working on learning autonomous vehicle behavior scoring function from human feedback. -->
                    Previously, I was a research intern at <a href="https://www.weride.ai/WeRide">WeRide</a>,
                    <a href="https://usa.honda-ri.com/">Honda Research Institute</a>, and <a
                      href="https://www.qualcomm.com/research/artificial-intelligence/ai-research">Qualcomm AI
                      Research</a>.
                  </p>

                  <!-- <p>
                    I sincerely appreciate the fellowship support from <a href="https://www.weride.ai/WeRide">WeRide</a>
                    (thank you! Dr. Yan Li, Dr. Hua Zhong, and Dr. Tony Han)
                    for funding my research in my first two years of study!
                  </p> -->

                  <!-- <p style="display:inline;color:#f18800;">I am actively looking for an industrial RS or a postdoctoral
                    position! Please reach out to me if you think I might be a good fit!</p> -->

                  <p style="text-align:left">
                    <!-- <a href="mailto:minae@cs.stanford.edu"><b>email</b></a> &nbsp&nbsp|&nbsp&nbsp -->
                    <!--<a href="minae_cv.pdf"><b>cv</b></a> &nbsp&nbsp|&nbsp&nbsp-->
                    <a href="https://scholar.google.com/citations?user=uY4D8-wAAAAJ&hl=en&authuser=1"><b>google
                        scholar</b></a> &nbsp&nbsp|&nbsp&nbsp
                    <a href="https://twitter.com/tianran_"><b>X</b></a> &nbsp&nbsp
                  </p>
                </td>
                <td style="padding:0%;width:15%;max-width:40%">
                  <a href="images/personal-image.jpg"><img style="width:100%;max-width:100%;border-radius:15%"
                      alt="profile photo" src="images/personal-image.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>

          </table>



          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding-left:2.5%;padding-bottom:20px;width:100%;vertical-align:middle">
                  <heading><b>Awards</b></heading>
                  <p>
                  <ul class="fa-ul">
                    <li><i class="fa fa-chevron-right"></i>
                      <p style="display:inline;color:#f18800;">[Feb 2025]</p> I was named a <a
                        href="https://robotics.umd.edu/futureleaders" style="color: red;"> Microsoft Future Leaders in
                        Robotics and AI </a>!
                    </li>
                    <li><i class="fa fa-chevron-right"></i>
                      <p style="display:inline;color:#f18800;">[Jan 2025]</p> I got <a style="color: red;"> Forbes 30
                        Under 30 </a> (Asia, tech) nomination!
                    </li>
                    <li><i class="fa fa-chevron-right"></i>
                      <p style="display:inline;color:#f18800;">[Jun 2024]</p> I was named a <a style="color: red;">
                        Rising Star </a> and won the
                      <a href=" https://www.thegaiaa.org/en/awards_mrzx#mrzx" style="color: red;"> Yunfan Award at the
                        World Artificial
                        Intelligence Conference </a> this year (top 15 early career Chinese AI researchers)!
                    </li>
                    <li><i class="fa fa-chevron-right"></i>
                      <p style="display:inline;color:#f18800;">[May 2024]</p> I was named a <a
                        href=" https://sites.google.com/view/rsspioneers2024/" style="color: red;"> Robotics: Science
                        and Systems
                        Pioneer</a> this year (top 30 early career researchers around the world in the robotics field)!
                    </li>
                    <li><i class="fa fa-chevron-right"></i>
                      <p style="display:inline;color:#f18800;">[Apr 2024]</p> I won the <a
                        href=" https://www.qualcomm.com/research/university-relations/innovation-fellowship/2024-north-america"
                        style="color: red;">
                        2024 Qualcomm Innovation Fellowship </a> (32 winners in North America)!
                    </li>
                    <li><i class="fa fa-chevron-right"></i>
                      <p style="display:inline;color:#f18800;">[Mar 2024]</p> I won the finalist award in the <a
                        href="http://scholarship.baidu.com/" style="color: red;"> Baidu AI Fellowship </a> this year
                      (top 20 Phd students in
                      the AI field around the world)!
                    </li>
                    <li><i class="fa fa-chevron-right"></i>
                      <p style="display:inline;color:#f18800;">[Mar 2024]</p> Our work <a
                        href=" https://robotics-transformer-x.github.io/"> RT-X</a> won the <a style="color: red;">2024
                        ICRA Best
                        Paper, Best Student Paper, and Best Manipulation Paper</a>!
                    </li>


                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <!-- news feed -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding-left:2.5%;padding-bottom:20px;width:100%;vertical-align:middle">
                  <heading><b>News</b></heading>
                  <p>
                  <ul class="fa-ul">
                    <li><img src="images/new_animated.gif" \="">
                      <p style="display:inline;color:#f18800;">[Feb 2025]</p> Our paper with Waymo on <a
                        href="https://openreview.net/forum?id=8UFG9D8xeU"> efficient post-training preference alignment
                        for motion generation</a> is accepted by ICLR as a <p style="display:inline;color:red;">
                        spotlight (notable top 5%) </p>.
                    </li>
                    <li><img src="images/new_animated.gif" \="">
                      <p style="display:inline;color:#f18800;">[Jan 2025]</p> I am co-organizing the <a
                        href=" https://sites.google.com/stanford.edu/safe-vlm-icra/home"> Safely Leveraging VLMs in
                        Robotics Workshop </a>
                      at ICRA with colleagues from CMU, Stanford, NVIDIA, Deepmind, Waymo, Anthropic, and MIT! Check out
                      our exciting program!
                    </li>
                    <li><img src="images/new_animated.gif" \="">
                      <p style="display:inline;color:#f18800;">[Jan 2025]</p> I am co-organizing the <a
                        href=" https://sites.google.com/stanford.edu/safe-vlm-icra/home"> RSS Pioneers Workshop </a>
                      at RSS this year!
                    </li>
                    <li> <img src="images/new_animated.gif" \="">
                      <p style="display:inline;color:#f18800;">[Jan 2025] </p>Ever watch your imitation-based robot
                      policy do
                      something bizarre? Wish you could fix it—no retraining needed? Meet <a
                        href="https://arxiv.org/abs/2502.01828"> FOREWARN</a>, a VLM-in-the-loop system
                      that steers multi-modal generative policies toward the right outcomes, on the fly!
                    </li>
                    <li> <img src="images/new_animated.gif" \="">
                      <p style="display:inline;color:#f18800;">[Jan 2025] </p>Not happy with your pre-trained robotics
                      foundation model?
                      Check out our paper <a href="https://arxiv.org/pdf/2412.04835"> Maximizing Alignment with Minimal
                        Feedback</a> to learn how we bring the success of preference alignment—popularized in
                      non-embodied foundation models (e.g., LLMs)—to robotics foundation models!
                    </li>
                    <li><i class="fa fa-chevron-right"></i>
                      <p style="display:inline;color:#f18800;">[Sep 2024]</p> Three papers accepted by CoRL! One on
                      using VLM to handle long-tail events in autonomous driving,
                      one on system-level failure detection for motion prediction model refinement,
                      and one on MoE policies for robot manipulation.
                    </li>
                    <li><i class="fa fa-chevron-right"></i>
                      <p style="display:inline;color:#f18800;">[Mar 2024]</p> Check out our new <a
                        href="https://arxiv.org/abs/2403.04745 ">preprint</a> on a general calibrated regret metric for
                      detecting and mitigating <b>system-level human-robot interaction failures</b> and how it can be
                      used to identify informative deployment data for efficiently improving behavior prediction models.
                    </li>
                    <li><i class="fa fa-chevron-right"></i>
                      <p style="display:inline;color:#f18800;">[Jan 2024]</p> Our work on visual representation
                      alignment for robot learning was accepted to ICLR 2024! We propose a tractable video-only method
                      for solving the visual representation alignment problem and learning visual robot rewards. Check
                      out the new results in the <a href="https://arxiv.org/pdf/2310.07932.pdf">arXiv version</a>.
                    </li>

                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding-left:20px;padding-bottom:0px;width:100%;vertical-align:middle">
                  <heading><b>Representative Publications</b></heading>

                  <p>For the most up-to-date list of publications, please see <a
                      href="https://scholar.google.com/citations?user=uY4D8-wAAAAJ&hl=en&authuser=1">google scholar</a>.
                    <br>
                    <!-- <p><i>* indicates equal contribution and co-authorship.</i></p> -->
                </td>
              </tr>
            </tbody>
          </table>




          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding-left:20px;padding-bottom:0px;width:100%;vertical-align:middle">
                  <b style="font-size: 1.3em;">Post-training Preference Alignment</b> <!-- Increased font size -->
                </td>
              </tr>
            </tbody>
          </table>

          <br>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

            <tbody>


              <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
                <img src="paper_imgs/demo_example_6_gt-ezgif.com-crop.gif" style="border-radius:5%/10%;width:200px">
              </td>
              <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
                <a href="https://arxiv.org/abs/2503.20105">
                  <papertitle>Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Model Using
                    Implicit Feedback from Pre-training Demonstrations
                  </papertitle>
                </a>
                <br>
                <b>Ran Tian</b>, Kratarth Goel
                <br>
                <em>International Conference on Learning Representations (ICLR), 2025,
                  <span class="highlight">
                    <p style="display:inline;color:#f9400d;">
                      Spotlight paper</p>
                  </span></em>
                <br>
                <br>
                <a class="button-paper" href="https://arxiv.org/abs/2503.20105"><i class="fa fa-file-text"
                    aria-hidden="true"></i> paper</a>
                &nbsp
                <a class="button-website"
                  href="https://sites.google.com/berkeley.edu/efficient-preference-alignment/home"><i
                    class="fa fa-external-link"></i> website</a>
                <p></p>
              </td>
            </tbody>

            <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
              <img src="paper_imgs/tian_ijrr_2025.gif" style="border-radius:5%/10%;width:200px">
            </td>
            <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <a href="https://arxiv.org/abs/2412.04835">
                <papertitle>Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor
                  Robot Policy Alignment
                </papertitle>
              </a>
              <br>
              <b>Ran Tian</b>, Yilin Wu, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, Andrea Bajcsy
              <br>
              <em>arXiv</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2412.04835"><i class="fa fa-file-text"
                  aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-website" href="https://sites.google.com/berkeley.edu/rapl"><i
                  class="fa fa-external-link"></i> website</a>
              <p></p>
            </td>
    </tbody>

    <tbody>
      <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
          src="paper_imgs/tian_rapl_23.png" style="border-radius:5%/10%;width:200px"></td>
      <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
        <a href="https://arxiv.org/abs/2310.07932">
          <papertitle>What Matters to You? Towards Visual Representation Alignment for Robot Learning
          </papertitle>
        </a>
        <br>
        <b>Ran Tian</b>, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, Andrea Bajcsy
        <br>
        <em>International Conference on Learning Representations (ICLR), 2024</em>
        <br>
        <br>
        <a class="button-paper" href="https://arxiv.org/abs/2310.07932"><i class="fa fa-file-text"
            aria-hidden="true"></i> paper</a>
        &nbsp
      </td>
    </tbody>
  </table>

  <br>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding-left:20px;padding-bottom:0px;width:100%;vertical-align:middle">
          <b style="font-size: 1.3em;">Onboard Deployment Efficiency and Safety</b> <!-- Increased font size -->
        </td>
      </tr>
    </tbody>
  </table>
  <br>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
        <img src="paper_imgs/wu_2024_forewarn.gif" style="border-radius:5%/10%;width:200px">
      </td>
      <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
        <a href="https://arxiv.org/abs/2502.01828">
          <papertitle>From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment
          </papertitle>
        </a>
        <br>
        Yilin Wu, <b>Ran Tian</b>, Gouku Swamy, Andrea Bajcsy
        <br>
        <em>arXiv</em>, 2025
        <br>
        <em>ICLR Workshop on World Models, 2025,
          <span class="highlight">
            <p style="display:inline;color:#f9400d;">
              Oral paper.</p>
          </span></em>
        <br>
        <br>
        <a class="button-paper" href="https://arxiv.org/abs/2502.01828"><i class="fa fa-file-text"
            aria-hidden="true"></i> paper</a>
        &nbsp
        <a class="button-website" href="https://yilin-wu98.github.io/forewarn/"><i class="fa fa-external-link"></i>
          website</a>
        <p></p>
      </td>
    </tbody>

    <tbody>
      <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/HRI_23.png"
          style="border-radius:5%/10%;width:200px"></td>
      <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
        <a href="https://arxiv.org/pdf/2301.00901.pdf">
          <papertitle>Towards Modeling and Influencing the Dynamics of Human Learning</papertitle>
        </a>
        <br>
        <b>Ran Tian</b>, Masayoshi Tomizuka, Anca Dragan, Andrea Bajcsy
        <br>
        <em>International Conference on Human-Robot Interaction (HRI), 2023
        </em>
        <br>
        <br>
        <a class="button-paper" href="https://arxiv.org/pdf/2301.00901.pdf"><i class="fa fa-file-text"
            aria-hidden="true"></i> paper</a>
        &nbsp
        <a class="button-talk" href="https://www.youtube.com/watch?v=KGsVm0qXDAc&t=3s"><i class="fa fa-volume-up"
            aria-hidden="true"></i> talk</a>
        <p></p>
      </td>

    </tbody>

    <tbody>
      <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
          src="paper_imgs/tian_2022.gif" style="border-radius:5%/10%;width:200px"></td>
      <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
        <a href="https://arxiv.org/abs/2109.14700">
          <papertitle>Safety Assurances for Human-Robot Interaction via Confidence-aware Game-theoretic Human Models
          </papertitle>
        </a>
        <br>
        <b>Ran Tian</b>, Liting Sun, Andrea Bajcsy, Masayoshi Tomizuka, Anca Dragan
        <br>
        <em>International Conference on Robotics and Automation (ICRA), 2022</em>
        <br>
        <br>
        <a class="button-paper" href="https://arxiv.org/abs/2109.14700"><i class="fa fa-file-text"
            aria-hidden="true"></i> paper</a>
        &nbsp
        <a class="button-talk"
          href="https://www.youtube.com/watch?v=YZlwMxepGtc&list=PLjbUVJgrbvfnyEXcYwEsTTLnsHu9_cuJa&index=6"><i
            class="fa fa-volume-up" aria-hidden="true"></i> talk</a>
        <p></p>
      </td>
    </tbody>
  </table>


  <br>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding-left:20px;padding-bottom:0px;width:100%;vertical-align:middle">
          <b style="font-size: 1.3em;">Representation and Policy Pre-training</b> <!-- Increased font size -->
        </td>
      </tr>
    </tbody>
  </table>
  <br>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
          src="paper_imgs/RT-X-vis.gif" style="border-radius:5%/10%;width:200px"></td>
      <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
        <a href="https://robotics-transformer-x.github.io/paper.pdf">
          <papertitle>Open X-Embodiment: Robotic Learning Datasets and RT-X Models</papertitle>
        </a>
        <br>
        Google, <b>Ran Tian</b>, et al.
        <br>
        <em>International Conference on Robotics and Automation (ICRA), 2024, <span class="highlight">
            <p style="display:inline;color:#f9400d;">
              Best paper, best student paper, and best manipulation paper</p>
          </span></em>
        <br>
        <br>
        <a class="button-paper" href="https://robotics-transformer-x.github.io/paper.pdf"><i class="fa fa-file-text"
            aria-hidden="true"></i> paper</a>
        &nbsp
        <a class="button-website" href="https://robotics-transformer-x.github.io/"><i class="fa fa-rss"
            aria-hidden="true"></i> website</a>
        <p></p>
      </td>
    </tbody>
    <tbody>

      <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
          src="paper_imgs/TOKEN_front_fig.png" style="border-radius:5%/10%;width:200px"></td>
      <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
        <a href="https://arxiv.org/abs/2407.00959">
          <papertitle>Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous
            Driving</papertitle>
        </a>
        <br>
        <b>Ran Tian</b>, Boyi Li, Xinshuo Weng, Yuxiao Chen, Edward Schmerling, Yue Wang, Boris Ivanovic, Marco
        Pavone
        <br>
        <em>Conference on Robot Learning (CoRL), 2024</em>
        <br>
        <a class="button-paper" href="https://arxiv.org/abs/2407.00959"><i class="fa fa-file-text"
            aria-hidden="true"></i> paper</a>
        &nbsp
        <a class="button-website" href="https://thomasrantian.github.io/TOKEN_MM-LLM_for_AutoDriving/"><i
            class="fa fa-rss" aria-hidden="true"></i> website</a>
      </td>
    </tbody>

    <tbody>

      <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img
          src="paper_imgs/huo_human_2023.gif" style="border-radius:5%/10%;width:200px"></td>
      <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
        <a href="https://sites.google.com/view/human-oriented-robot-learning">
          <papertitle>Human-oriented Representation Learning for Robotic Manipulation</papertitle>
        </a>
        <br>
        Mingxiao Huo, Mingyu Ding, Chenfeng Xu, <b>Ran Tian</b>, Xinghao Zhu, Yao Mu Lingfeng Sun, Masayoshi
        Tomizuka,
        Wei Zhan
        <br>
        <em> Robotics: Science and Systems, 2024</em>
        <br>
        <br>
        <a class="button-paper" href="https://arxiv.org/pdf/2310.03023.pdf"><i class="fa fa-file-text"
            aria-hidden="true"></i> paper</a>
        &nbsp
        <a class="button-website" href="https://sites.google.com/view/human-oriented-robot-learning"><i
            class="fa fa-rss" aria-hidden="true"></i> website</a>
        <p></p>
      </td>
    </tbody>
  </table>

  </tbody>
  </table>








  <hr>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0" style="padding-top:0px;">
    <tbody>
      <tr>
        <td>
          <br>
          <p align="center" style="font-size:small;color:grey;">
            website adapted from <a href="https://people.eecs.berkeley.edu/~abajcsy/" style="font-size:small;">here</a>
            </font>
          </p>
        </td>
      </tr>
    </tbody>
  </table>

</body>

</html>